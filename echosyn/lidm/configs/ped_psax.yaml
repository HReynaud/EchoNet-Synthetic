wandb_group: "lidm"
output_dir: experiments/lidm_ped_psax

pretrained_model_name_or_path: null
vae_path: models/vae

globals:
    target_fps: "original"
    target_nframes: 32
    outputs: ["image"]

datasets:
    - name: Latent
      active: true
      params:
        root: data/latents/ped_psax
        target_fps: ${globals.target_fps}
        target_nframes: ${globals.target_nframes}
        target_resolution: 14 # emb resolution
        outputs: ${globals.outputs}

unet:
    _class_name: UNet2DModel
    sample_size: 14 # actual size is 16
    in_channels: 4
    out_channels: 4
    center_input_sample: false
    time_embedding_type: positional
    freq_shift: 0
    flip_sin_to_cos: true
    down_block_types: 
        - AttnDownBlock2D
        - AttnDownBlock2D
        - AttnDownBlock2D
        - DownBlock2D
    up_block_types: 
        - UpBlock2D
        - AttnUpBlock2D
        - AttnUpBlock2D
        - AttnUpBlock2D
    block_out_channels: 
        - 128
        - 256
        - 256
        - 512
    layers_per_block: 2
    mid_block_scale_factor: 1
    downsample_padding: 1
    downsample_type: resnet
    upsample_type: resnet
    dropout: 0.0
    act_fn: silu
    attention_head_dim: 8
    norm_num_groups: 32
    attn_norm_num_groups: null
    norm_eps: 1e-05
    resnet_time_scale_shift: "default"
    class_embed_type: null
    num_class_embeds: null

noise_scheduler:
    _class_name: DDPMScheduler
    num_train_timesteps: 1000
    beta_start: 0.0001
    beta_end: 0.02
    beta_schedule: linear # linear, scaled_linear, or squaredcos_cap_v2
    variance_type: fixed_small # fixed_small, fixed_small_log, fixed_large, fixed_large_log, learned or learned_range
    clip_sample: true
    clip_sample_range: 4.0 # default 1 
    prediction_type: v_prediction # epsilon, sample, v_prediction
    thresholding: false # do not touch
    dynamic_thresholding_ratio: 0.995 # unused
    sample_max_value: 1.0 # unused
    timestep_spacing: "leading" #
    steps_offset: 0 # unused

train_batch_size: 256
dataloader_num_workers: 16
max_train_steps: 500000

learning_rate: 3e-4
lr_warmup_steps: 500
scale_lr: false
lr_scheduler: constant
use_8bit_adam: false
gradient_accumulation_steps: 1

noise_offset: 0.0

gradient_checkpointing: false
use_ema: true
enable_xformers_memory_efficient_attention: false
allow_tf32: true

adam_beta1: 0.9
adam_beta2: 0.999
adam_weight_decay: 1e-2
adam_epsilon: 1e-08
max_grad_norm: 1.0

logging_dir: logs
mixed_precision: "fp16" # "no", "fp16", "bf16"

validation_timesteps: 128
validation_fps: ${globals.target_fps}
validation_frames: ${globals.target_nframes}
validation_count: 4 # defines the number of samples
validation_guidance: 1.0
validation_steps: 2500

report_to: wandb
checkpointing_steps: 10000 # ~3/hour
checkpoints_total_limit: 100 # no limit
resume_from_checkpoint: latest
tracker_project_name: echosyn

seed: 42

